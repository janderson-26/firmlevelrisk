---
title: "bartik_1"
author: "Janel"
date: "2025-10-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r}
rm(list = ls())
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(haven)
library(ggplot2)
library(fredr)
library(ggplot2)
library(reshape)

fredr_set_key("58c324a0411b09b8bc5d2a2949f6b5b1") 
firmquarter_2022q1 <- read_dta("~/Downloads/firmquarter_2022q1.dta") 
cbp17co <- read.csv("~/Downloads/cbp17co.txt")
ccd_gvkey_naics <- read_dta("~/Downloads/ccd_gvkey_naics.dta")
zhvi_long1625 <- read.csv("~/Pen world/zhvi_long1625.csv")

```



combine firm and naics codes
```{r}

# Merge firm on gvkey
merged_data <- merge(
  firmquarter_2022q1, 
  ccd_gvkey_naics, 
  by = "gvkey",         # common key
  all.x = TRUE          # keep all firm_risk rows (left join)
)

# Inspect result
head(merged_data)

```

narrow data to relivant date/s. In this case Q1 2022. = 248
```{r}
firm <- merged_data |>
  filter(date == 248)
#firm <- merged_data |>
 # filter(between(date, 238, 248))
#Q419
#firm230 <-firm %>%
 # filter(date == 240)
#Q422
#firm240 <-firm %>%
  #filter(date == 248)


  
```

select relevant variables
```{r}
selected_firm<- firm |>
  select(naics,company_name.y,date, Covid_Exposure,Covid_Neg_Sentiment,Covid_Pos_Sentiment, Covid_Net_Sentiment, Covid_Risk,PRisk,PRiskT_economic,PRiskT_trade, PRiskT_health, PRiskT_institutions, NPRisk, Risk, Sentiment, PSentiment, NPSentiment)

#zhvi_long19_25 <- zhvi_long1625 |>
  #select(RegionName,StateName, StateCodeFIPS, MunicipalCodeFIPS, date, zhvi) |>
  #filter(between(date, 2019-01-31, 2025-05-30))
```

3 digit naics 
```{r}
selected_firm <- selected_firm %>%
  mutate(
    naics = str_replace_all(as.character(naics), "\\D", ""),   # drop non-digits
    naics3      = if_else(nchar(naics) >= 3, substr(naics, 1, 3), NA_character_)
  )
selected_firm |>
count(naics3) 
```
mean of covid exposure across all sectors w/ respect to time
```{r}

date_naics <- selected_firm %>%
  filter(!is.na(naics3), !is.na(date)) %>%
  group_by(date, naics3) %>%
  summarise(
    n = n(),
    across(
      .cols = where(is.numeric) & !any_of("date"),
      .fns  = list(mean = ~ mean(.x, na.rm = TRUE),
                   median = ~ median(.x, na.rm = TRUE)),
      .names = "{.col}_{.fn}"
    ),
    .groups = "drop"
  ) %>%
  arrange(date, naics3)

# Inspect
glimpse(date_naics)
head(date_naics)

```

county business patterns 2017 clean up naics to 3 digit
```{r}

cbp173 <- cbp17co %>%
  mutate(naics_digits = str_replace_all(as.character(naics), "\\D", "")) %>%
  # keep 1–3 digits; drop blanks and 4+ digits
  filter(str_detect(naics_digits, "^[0-9]{1,3}$")) %>%
  mutate(naics3 = str_pad(naics_digits, width = 3, side = "right"))

```

Merge firm risk and county on naics
```{r}
# ensure key types match
date_naics <- date_naics %>% mutate(naics3 = as.character(naics3))
cbp17  <- cbp173 %>% mutate(naics3 = as.character(naics3))
merged <- date_naics %>%
  left_join(cbp17, by = "naics3")  # keep all rows from date_naics

```

```{r}
#state_naics <- state_naics |>
 # drop_na(fipstate)

state_naics <- merged %>%
  group_by(fipstate) %>%
  mutate(
    state_total_emp     = sum(emp, na.rm = TRUE), #total emp of state
    state_industry_share = emp / state_total_emp,
    state_covid_ex = (state_industry_share * Covid_Exposure_mean)
    
  ) 


  
# Inspect
head(state_naics)

```

```{r}
write.csv(state_naics, "state_naics.csv", row.names = FALSE, na = "")

```



code by county composition of naics time INVARIENT 
```{r}

# Ensure zero-padded FIPS and keep original columns
merged_county <- merged %>%
  mutate(
    fipstate = str_pad(as.character(fipstate), width = 2, side = "left", pad = "0"),
    fipscty  = str_pad(as.character(fipscty),  width = 3, side = "left", pad = "0"),
    fips5    = paste0(fipstate, fipscty)
  )
head(merged_county)
```
county total employment 
```{r}
# ---- 2) County totals ----
county_totals <- merged_county %>%
  group_by(fips5) %>%
  mutate(county_total_emp = sum(emp, na.rm = TRUE)) |>
  ungroup()
```

```{r}

# ---- 3) industry Composition (shares within county) ----
county_naics_comp_long <- county_totals %>%
  mutate(
    emp_share = if_else(county_total_emp > 0, emp / county_total_emp, NA_real_)
  ) %>%
ungroup()

# Long result: one row per county × NAICS3 with shares
head(county_naics_comp_long)


```
County industry share * covid exsposure mean
```{r}

county_industry_contrib <- county_naics_comp_long %>%
  mutate(
    across(
      .cols = where(is.numeric) & ends_with("_mean"),
      .fns  = ~ .x * emp_share,
      .names = "{.col}_contrib"
    )
  )

```

```{r}


county_exposure <- county_industry_contrib %>%
  group_by(fips5) %>%
  summarise(
    # sum all contribution columns
    across(ends_with("_contrib"), ~ sum(.x, na.rm = TRUE)),
    # keep any county-level totals you want (optional)
    county_total_emp = dplyr::first(county_total_emp),
    .groups = "drop"
  )

head(county_exposure)

```


zillow data
```{r}

zhvi_long1625 <- zhvi_long1625 %>%
  mutate(
    fipstate = str_pad(as.character(StateCodeFIPS), width = 2, pad = "0"),
    fipscty  = str_pad(as.character(MunicipalCodeFIPS), width = 3, pad = "0"),
    fips5    = paste0(fipstate, fipscty)
  )

```

```{r}
cleanz<- zhvi_long1625 |>
  select(RegionName,State, date, zhvi, fips5)

```

```{r}

annual_zhvi <- cleanz %>%
  select(RegionName, State, date, zhvi, fips5) %>%
  mutate(
    date = ymd(date),
    year = year(date)
  ) %>%
  group_by(fips5, year) %>%
   summarise(
    RegionName = first(RegionName),
    State      = first(State),
    zhvi_avg   = mean(zhvi, na.rm = TRUE),
    n_months   = sum(!is.na(zhvi)),   # how many months used
    .groups = "drop"
  )

```



```{r}
merged <- county_exposure %>%
  left_join(annual_zhvi, by = "fips5")
```

```{r}


# 1) Keep only 2017 and 2023 exposure; pivot wide
covid_wide <- merged %>%
  filter(year %in% c(2017, 2023)) %>%
  pivot_wider(names_from = year, values_from = zhvi_avg,
              names_prefix = "zhvi_")  # columns: zhvi_2017, zhvi_2023

```

```{r}
covid_wide <- covid_wide |>
  mutate(
    # outcomes you might use
    diff_zhvi = zhvi_2023 - zhvi_2017,                             # level change
    diffln_zhvi = log(zhvi_2023) - log(zhvi_2017)                  # log change (pct approx)
  )
head(covid_wide)
```

```{r}


num_df <- covid_wide %>% select(where(is.numeric))
cor_mat <- cor(num_df, use = "pairwise.complete.obs", method = "pearson")
round(cor_mat, 3)

```
```{r}
m1 <- lm(diffln_zhvi ~ Covid_Exposure_mean_contrib , data = covid_wide)
stargazer(
  m1,
  type = "text")
```

```{r}
m1 <- lm(diff_zhvi ~ Covid_Exposure_mean_contrib, data = covid_wide)
stargazer(
  m1,
  type = "text")
```

```{r}
m1 <- lm(zhvi_2023 ~ Covid_Exposure_mean_contrib, data = covid_wide)
stargazer(
  m1,
  type = "text")
```

```{r}
housing_starts <- fredr(
  series_id = "HOUST",
  observation_start = as.Date("1993-01-01"),
  observation_end = as.Date("2024-01-01")
)
```


  # Heuristics: keep annual, not seasonally adjusted, minutes, Census source; best title match first
  srch <- srch %>%
    arrange(desc(stringr::str_detect(title, fixed(q, ignore_case = TRUE))),
            desc(str_detect(units, "Minutes")),
            frequency_short,
            desc(observation_end)) %>%
    filter(
      str_detect(title, fixed(county_name, ignore_case = TRUE)),
      str_detect(title, fixed(state_abbr,  ignore_case = TRUE))
    )
  
  if (nrow(srch) == 0) {
    return(tibble(
      fips5 = fips5,
      county_name = county_name,
      state_abbr = state_abbr,
      series_id = NA_character_,
      date = as.Date(NA),
      commute_minutes = NA_real_
    ))
  }
  
  sid <- srch$series_id[1]
  
  obs <- fredr_series_observations(series_id = sid)
  if (nrow(obs) == 0) {
    return(tibble(
      fips5 = fips5,
      county_name = county_name,
      state_abbr = state_abbr,
      series_id = sid,
      date = as.Date(NA),
      commute_minutes = NA_real_
    ))
  }
  
  latest <- obs %>% arrange(desc(date)) %>% slice(1)
  tibble(
    fips5 = fips5,
    county_name = county_name,
    state_abbr = state_abbr,
    series_id = sid,
    date = latest$date,
    commute_minutes = as.numeric(latest$value)
  )
}

# -------------------------------------------------------------------
# Vectorized fetch for a counties table you already have
# Expect a data frame `counties` with columns: fips5, county_name, state_abbr
# (If you have fipstate + fipscty instead, create fips5 = paste0(pad2, pad3))
# -------------------------------------------------------------------

# EXAMPLE scaffold: adapt to your data
# counties <- your_county_df %>%
#   transmute(
#     fips5 = stringr::str_pad(as.character(fips5), 5, pad = "0"),
#     county_name = RegionName,       # e.g., "Alameda County"
#     state_abbr  = State             # e.g., "CA"
#   ) %>%
#   distinct(fips5, .keep_all = TRUE)

# Fetch (be mindful of rate limits; this will do one API call per county)
commute_by_county <- counties %>%
  mutate(fips5 = str_pad(as.character(fips5), 5, pad = "0")) %>%
  distinct(fips5, .keep_all = TRUE) %>%
  mutate(row_id = row_number()) %>%
  # map rows safely
  pmap_dfr(function(fips5, county_name, state_abbr, row_id, ...) {
    # polite tiny delay helps avoid throttling on very large lists
    if (row_id %% 60 == 0) Sys.sleep(1)
    fetch_commute_one(fips5, county_name, state_abbr)
  }) %>%
  select(fips5, date, commute_minutes, series_id)



```

